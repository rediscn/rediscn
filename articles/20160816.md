---
layout: articles
title: 每天近百亿条用户数据，携程大数据高并发应用架构涅槃
permalink: articles/20160816.html
disqusIdentifier: articles/20160816.html
disqusUrl: http://redis.cn/monthly/temp.html
discuzTid: 
---


互联网二次革命的移动互联网时代，如何吸引用户、留住用户并深入挖掘用户价值，在激烈的竞争中脱颖而出，是各大电商的重要课题。通过各类大数据对用户进行研究，以数据驱动产品是解决这个课题的主要手段，携程的大数据团队也由此应运而生；经过几年的努力，大数据的相关技术为业务带来了惊人的提升与帮助。

以基础大数据的用户意图服务为例，通过将广告和栏位的“千人一面”变为“千人千面”，在提升用户便捷性，可用性，降低费力度的同时，其转化率也得到了数倍的提升，体现了大数据服务的真正价值。

在新形势下，传统应用架构不得不变为大数据及新的高并发架构，来应对业务需求激增及高速迭代的需要。

**一，业务高速发展带来的应用架构挑战**

公司业务高速发展带来哪些主要的变化，以及给我们的系统带来了哪些挑战？  

**1）****业务需求的急速增长**，访问请求的并发量激增，2016年1月份以来，业务部门的服务日均请求量**激增了5.5倍**。

**2）****业务逻辑日益复杂化**，基础业务研发部需要支撑起OTA数十个业务线，业务逻辑日趋复杂和繁多。

**3）****业务数据源多样化，异构化**，接入的业务线、合作公司的数据源越来越多；接入的数据结构由以前的数据库结构化数据整合转为Hive表、评论文本数据、日志数据、天气数据、网页数据等多元化异构数据整合。

**4）****业务的高速发展和迭代**，部门一直以追求以最少的开发人力，以架构和系统的技术优化，支撑起携程各业务线高速发展和迭代的需要。

在这种新形势下，传统应用架构不得不变，做为工程师也必然要自我涅槃，改为大数据及新的高并发架构，来应对业务需求激增及高速迭代的需要。计算分层分解、去SQL、去数据库化、模块化拆解的相关技改工作已经刻不容缓。

以用户意图（AI 点金杖）的个性化服务为例， 面对BU业务线的全面支持的迫切需要，其应用架构必须解决如下**技术难点**：

**1）高访问并发**：每天近亿次的访问请求；

**2）数据量大**：每天TB级的增量数据，近百亿条的用户数据，上百万的产品数据；

**3）业务逻辑复杂**：复杂个性化算法和LBS算法；例如：满足一个复杂用户请求需要大量计算和30次左右的SQL数据查询，服务延时越来越长；

**4）高速迭代上线**：面对OTA多业务线的个性化、Cross-saling、Up-saling、需满足提升转化率的迫切需求，迭代栏位或场景要快速，同时减少研发成本。

**二，应对挑战的架构涅磐**

面对这些挑战，我们的应用系统架构应该如何涅磐？主要分如下**三大方面**系统详解:  

  

**存储的涅磐**，这一点对于整个系统的吞吐量和并发量的提升起到最关键的作用，需要结合数据存储模型和具体应用的场景。

**计算的涅磐**，可以从横向和纵向考虑：横向主要是增加并发度，首先想到的是分布式。纵向拆分就是要求我们找到计算的结合点从而进行分层，针对不同的层次选择不同的计算地点。然后再将各层次计算完后的结果相结合，尽可能最大化系统整体的处理能力。

**业务层架构的涅磐**，要求系统的良好的模块化设计，清楚的定义模块的边界，模块自升级和可配置化。

**三，应用系统的整体架构**

认识到需要应对的挑战，我们应该如何设计我们的系统呢，下面将全面的介绍下我们的应用系统整体架构。  

下图就是我们应用系统整体架构以及系统层次的模块构成。

![](https://mmbiz.qlogo.cn/mmbiz_png/cokWkYcF4Dcib3BRiaOicg2NWADK6L9eo4n6nXVv4TEDXCbwIxuJhKYIUOsvsXK5R4yLIXKdTMJOcjIeGxFJiaonAg/?wx_fmt=png)  

**数据源部分**， Hermes是携程框架部门提供的消息队列，基于Kafka和Mysql做为底层实现的封装，应用于系统间实时数据传输交互通道。Hive和 HDFS是携程海量数据的主要存储，两者来自Hadoop 生态体系。Hadoop 这块大家已经很熟悉， 如果不熟悉的同学只要知道Hadoop 主要用于大数据量存储和并行计算批处理工作。

Hive 是基于Hadoop平台的数据仓库，沿用了关系型数据库的很多概念。比如说数据库和表，还有一套近似于SQL的查询接口的支持，在Hive里 叫做HQL，但是其底层的实现细节和关系型数据库完全不一样，Hive底层所有的计算都是基于MR来完成，我们的数据工程师90%都数据处理工作都基于它来完成。

**离线部分**，包含的模块有MR, Hive , Mahout, SparkQL/MLLib。Hive 上面已经介绍过，Mahout 简单理解提供基于Hadoop平台进行数据挖掘的一些机器学习的算法包。Spark类似hadoop也是提供大数据并行批量处理平台，但是它是基于内存的。SparkQL 和Spark MLLib是基于Spark平台的SQL查询引擎和数据挖掘相关算法框架。我们主要用Mahout和Spark MLLib 进行数据挖掘工作。

**调度系统zeus**，是淘宝开源大数据平台调度系统，于2015年引进到携程，之后我们进行了重构和功能升级，做为携程大数据平台的作业调度平台。

**近线部分**，是基于Muise来实现我们的近实时的计算场景，Muise是也是携程OPS提供的实时计算流处理平台，内部是基于Storm实现与HERMES消息队列搭配起来使用。例如，我们使用MUSIE通过消费来自消息队列里的用户实时行为，订单记录，结合画像等一起基础数据，经一系列复杂的规则和算法，实时的识别出用户的行程意图。

**后台/线上应用部分**，Mysql用于支撑后台系统的数据库。ElasticSearch 是基于Lucene实现的分布式搜索引擎，用于索引用户画像的数据，支持离线精准营销的用户筛选，同时支持线上应用推荐系统的选品功能 。Hbase 基于Hadoop的Hdfs 上的列存储Nosql数据库，用于后台报表可视化系统和线上服务的数据存储。

这里说明一下, 在线和后台应用使用的ElasticSearch和Hbase集群是分开的，互不影响。 Redis 支持在线服务的高速缓存，用于缓存统计分析出来的热点数据。

**四，推荐系统案例**

介绍完我们应用系统的整体构成， 接下来分享基于这套系统架构实现的一个实例——携程个性化推荐系统。  

推荐系统的架构图：

![](https://mmbiz.qlogo.cn/mmbiz_png/cokWkYcF4Dcib3BRiaOicg2NWADK6L9eo4nvdexsmrLUIvEH5WRlfMSsbxVhhepvB80b6licZZdhia97lCYia0ghNbQA/?wx_fmt=png)  

1

**存储的涅磐**

**1）Nosql (Hbase+Redis)**  

![](https://mmbiz.qlogo.cn/mmbiz_png/cokWkYcF4Dcib3BRiaOicg2NWADK6L9eo4nXkick4lSGvEdoDwYh040PHzUGNicG4KgK1GCTnmveE6Qb8vfNK0HZEfw/?wx_fmt=png)  

我们之前存储使用的是Mysql,  一般关系型数据库会做为应用系统存储的首选。大家知道Mysql非商业版对分布式支持不够，在存储数据量不高，查询量和计算复杂度不是很大的情况下，可以满足应用系统绝大部分的功能需求。

我们现状是需要安全存储海量的数据，高吞吐，并发能力强，同时随着数据量和请求量的快速增加，能够通过加节点来扩容。另外还需要支持故障转移，自动恢复，无需额外的运维成本。综上几个主要因素，我们进行了大量的调研和测试，最终我们选用Hbase和Redis 两个Nosql数据库来取代以往使用的Mysql,。我们把用户意图以及推荐产品数据以KV的形式存储在Hbase中，我对操作Hbase进行一些优化，其中包括rowkey的设计，预分配，数据压缩等， 同时针对我们的使用场景对Hbase本身配置方面的也进行了调优。目前存储的数据量已经达到TB级别，支持每天千万次请求，同时保证99%在50毫秒内返回 。

Redis这块和多数应用系统使用方式一样，主要用于缓存热点数据，这里就不多说了。  

![](https://mmbiz.qlogo.cn/mmbiz_png/cokWkYcF4Dcib3BRiaOicg2NWADK6L9eo4nspiaicSg104ZM4FstfDMFGfd4ILJ8Wpy2cIVXOvPnHibH1RJG6DmEkmFw/?wx_fmt=png)  

**2）搜索引擎 (ElasticSearch)**

![](https://mmbiz.qlogo.cn/mmbiz_png/cokWkYcF4Dcib3BRiaOicg2NWADK6L9eo4nlKAaF1GFg2mBnTqCq1gZds7kyccyyxuoQJR3bN17ZraXdQclCgIgBA/?wx_fmt=png)  

ES索引各业务线产品特征数据，提供基于用户的意图特征和产品特征复杂的多维检索和排序功能，当前集群由4台大内存物理机器构成，采用全内存索引。对比某一个复杂的查询场景， 之前用Mysql将近需要30次查询，使用ES只需要一次组合查询且在100毫秒内返回 。目前每天千万次搜索，99%以上在300毫秒以内返回。

![](https://mmbiz.qlogo.cn/mmbiz_png/cokWkYcF4Dcib3BRiaOicg2NWADK6L9eo4n8IJnzfYllncibXu4xHwNic00cXic9T7T4XPxVuynEicmk7bzMcRCLKLFDg/?wx_fmt=png)  

  

2

**计算的涅磐**

**1）数据源**，我们的数据源分结构化和半结构化数据以及非结构化数据。

结构化数据主要是指携程各产线的产品维表和订单数据，有酒店，景酒，团队游，门票，景点等；还有一些基础数据，比如城市表，车站等，这类数据基本上都是T+1。每天会有流程去各BU的生产表拉取数据。

半结构化数据是指，携程用户的访问行为数据，例如浏览，搜索，预订，反馈等，这边顺便提一下，这些数据这些是由前端采集框架实时采集，然后下发到后端的收集服务，由收集服务在写入到Hermes消息队列，一路会落地到Hadoop上面做长期存储，另一路近线层可以通过订阅Hermes此类数据Topic 进行近实时的计算工作。

我们还用到外部合作渠道的数据，还有一些评论数据，评论属于非结构化的，也是T+1更新。

**2）离线计算**，主要分三个处理阶段 。

![](https://mmbiz.qlogo.cn/mmbiz_png/cokWkYcF4Dcib3BRiaOicg2NWADK6L9eo4nACfjlIKc35LeDGUxgJSyapEVrF0x0kWWHIOoJNp9aw28ryFJHcz0ZQ/?wx_fmt=png)  

**预处理阶段**，这块主要为后续数据挖掘做一些数据的准备工作，数据去重，过滤，对缺失信息的补足。举例来说采集下来的用户行为数据，所含有的产品信息很少，我们会使用产品表的数据进行一些补足，确保给后续的数据挖掘使用时候尽量完整的。

**数据挖掘阶段**，主要运用一些常用的数据挖掘算法进行模型训练和·推荐数据的输出(分类，聚类，回归 ，CF等)。

**结果导入阶段**，我们通过可配置的数据导入工具将推荐数据，进行一系列转换后，导入到HBASE，Redis以及建立ES索引， Redis存储的是经统计计算出的热点数据 。 

**3）近线计算（用户意图, 产品缓存）**

当用户没有明确的目的性情况下，很难找到满足兴趣的产品，我们不仅需要了解用户的历史兴趣，用户实时行为特征的抽取和理解更加重要，以便快速的推荐出符合用户当前兴趣的产品，这就是用户意图服务需要实现的功能。

一般来说用户特征分成两大类：一种是稳定的特征（用户画像），如用户性别，常住地，主题偏好等特征；另一类是根据用户行为计算获取的特征，如用户对酒店星级的偏好，目的地偏好，跟团游/自由行偏好等。基于前面所述的计算的特点，我们使用近在线计算来获取第二类用户特征，整体框图如下。从图中可以看出它的输入数据源包括两大类：第一类是实时的用户行为，第二类是用户画像，历史交易以及情景等离线模块提供的数据。结合这两类数据，经一些列复杂的近线学习算法和规则引擎，计算得出用户当前实时意图列表存储到Hbase和Redis中。

![](https://mmbiz.qlogo.cn/mmbiz_png/cokWkYcF4Dcib3BRiaOicg2NWADK6L9eo4nzx1N3V5ibGth7T0YcnkvbKd0hxWiblKMM9Gqs6wXEByF280k8uudpaUQ/?wx_fmt=png)  

携程用户意图框架

近线另一个工作是产品数据缓存，携程的业务线很多，而我们的推荐系统会推各个业务线的产品，因此我们需要调用所有业务线的产品服务接口，但随着我们上线的场景的增加，这样无形的增加了对业务方接口的调用压力。而且业务线产品接口服务主要应用于业务的主流程或关键型应用，比较重，且SLA服务等级层次不齐，可能会影响到整个推荐系统的响应时间。

为了解决这两个问题，我们设计了近在线计算来进行业务的产品信息异步缓存策略，具体的流程如下。

我们会将待推荐的产品Id全部通过Kafka异步下发，在Storm中我们会对各业务方的产品首先进行聚合，达到批处理个数或者时间gap时，再调用各业务方的接口，这样减少对业务方接口的压力。通过调用业务方接口更新的产品状态临时缓存起来（根据各业务产品信息更新周期分别设置缓存失效时间），在线计算的时候直接先读取临时缓存数据，缓存不存在的情况下，再击穿到业务的接口服务。

![](https://mmbiz.qlogo.cn/mmbiz_png/cokWkYcF4Dcib3BRiaOicg2NWADK6L9eo4nR1tQeWq125FKBmJrD2VCwNhw714NI3qbRI7lKtTJ8nPcpmicRdo67jw/?wx_fmt=png)  

产品异步缓存框架

**4）在线计算（2个关键业务层架构模块介绍）**

**1，业务层架构-数据治理和访问模块**，支持的存储介质，目前支持的存储介质有Localcache，Redis，Hbase，Mysql 可以支持横向扩展 。统一配置，对同一份数据，采用统一配置，可以随意存储在任意介质，根据id查询返回统一格式的数据，对查询接口完全透明。

穿透策略和容灾策略，Redis只存储了热数据，当需要查询冷数据则可以自动到下一级存储如Hbase查询，避免缓存资源浪费。当Redis出现故障时或请求数异常上涨，超过整体承受能力，此时服务降级自动生效，并可配置化。 

![](https://mmbiz.qlogo.cn/mmbiz_png/cokWkYcF4Dcib3BRiaOicg2NWADK6L9eo4nCwHy4N01q6icP0dAiamGz2lDT4VfU8xBNpJ6iaRQ1gZFeoN9tP9oZ9wXw/?wx_fmt=png)  

**2，业务层架构-推荐策略模块**，整个流程是先将用户意图、用户浏览，相关推荐策略生成的产品集合等做为数据输入，接着按照场景规则，业务逻辑重新过滤，聚合、排序。最后验证和拼装业务线产品信息后输出推荐结果；![](https://mmbiz.qlogo.cn/mmbiz_png/cokWkYcF4Dcib3BRiaOicg2NWADK6L9eo4nPjTlEU8rJ2M2NbDzkPVad8kZAkSfV0ERwXlBRX97icAJzldKJM2lahA/?wx_fmt=png)  

我们对此流程每一步进行了一些模块化的抽象，将重排序逻辑按步骤抽象解耦，抽象如右图所示的多个组件，开发新接口时仅需要将内部DSL拼装便可以得到满足业务需求的推荐服务；提高了代码的复用率和可读性，减少了超过50%的开发时间；对于充分验证的模块的复用，有效保证了服务的质量；

作者介绍

董锐，近9年的互联网从业经验。2013年1月加入携程，曾在商业智能部设计和开发基于HADOOP生态体系的大数据数据仓库。现任基础业务研发部-数据智能应用组研发Leader，专注于携程个性化推荐系统，ABTEST等系统研发工作。

**大数据杂谈 **  

_ID：BigdataTina2016_

![](https://mmbiz.qlogo.cn/mmbiz/cokWkYcF4DeHXfNoSgu83JHV73NZ2wXonh5sajIDYhliaOmnufAjdYGMEbY2Eb3ZOfxeC1ibAj29Ny1X0qhibYiaWQ/?wx_fmt=jpeg)  
▲长按二维码识别关注

专注大数据和机器学习，

分享前沿技术，交流深度思考。

欢迎加入社区！